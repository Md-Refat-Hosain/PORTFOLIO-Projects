
⚙️ Technical AspectsThis section details the technical methodologies and tools employed throughout the Car Price Prediction project, from data ingestion to model deployment.1. Data Processing & Feature EngineeringSource Data: The project utilized a comprehensive dataset of car listings, including attributes such as mileage, horsepower, age, transmission details, and various categorical features like brand, model, exterior/interior colors, and fuel type.Data Cleaning:Handled missing values through appropriate imputation strategies (e.g., median for numerical features, mode for categorical features).Identified and addressed outliers in numerical features (e.g., price, mileage, horsepower) to improve model robustness.Feature Engineering:Created new features where applicable (e.g., Age from year of manufacture, if not directly available).Managed complex categorical features, including those with special characters like 'Plug-In Hybrid' and '–', ensuring they were correctly processed for model training.Data Transformation:Applied log transformation to the target variable (car price) to mitigate skewness and stabilize variance, which significantly improved model performance.Used One-Hot Encoding for nominal categorical features and potentially Target Encoding / Label Encoding for high-cardinality features to convert them into a numerical format suitable for machine learning models.Standardization/Normalization was applied to numerical features to scale them consistently.2. Model Development & EvaluationModel Selection: Explored a range of regression models to identify the best performer for the car price prediction task:Linear Regression: Established a baseline understanding of linear relationships.Random Forest Regressor: Leveraged ensemble learning for robust predictions and feature importance insights.XGBoost Regressor (Extreme Gradient Boosting): Employed a highly optimized gradient boosting framework known for its performance on tabular data.Hyperparameter Tuning: Utilized GridSearchCV with K-Fold Cross-Validation to systematically search for the optimal hyperparameters for the best-performing model, ensuring robust generalization and preventing overfitting.Best Performing Model: The Hyperparameter Tuned XGBoost Regressor emerged as the top performer, demonstrating superior predictive accuracy compared to other models.Evaluation Metrics: Model performance was rigorously assessed using:R-squared (R2): Measures the proportion of variance in the dependent variable that can be predicted from the independent variables. Achieved an R2 of 0.8072 (on log-transformed prices) with the best XGBoost model.Mean Absolute Error (MAE): Represents the average magnitude of the errors in a set of predictions, without considering their direction. Achieved an MAE of $17,641.91 (on original price scale).Root Mean Squared Error (RMSE): A measure of the differences between values predicted by a model and the values actually observed. The RMSE of $154,451.56 (on original price scale) indicated the presence of a few very large errors, primarily on high-value outlier cars, which was further investigated through residual analysis.3. Model Persistence & Deployment (MLOps)Model Persistence: The final, best-performing XGBoost model was saved using joblib to facilitate its loading and use in a production environment.API Development with FastAPI:Developed a RESTful API using FastAPI to serve real-time car price predictions.Defined a Pydantic BaseModel for input data validation, ensuring that incoming requests conform to the expected feature schema and data types. This robustly handles data integrity at the API endpoint.Implemented careful handling for feature names containing special characters (e.g., 'Plug-In Hybrid', '–') by mapping Pydantic's valid Python attribute names to the exact column names expected by the trained model.Deployment in Google Colab with ngrok:The FastAPI application was deployed and run directly within a Google Colab environment.pyngrok was utilized to create a secure, publicly accessible tunnel to the local FastAPI server, enabling external access and testing of the API.threading was employed to run the uvicorn server in a separate thread, resolving event loop conflicts common in Jupyter/Colab environments and ensuring stable API operation.Interactive API Documentation: FastAPI automatically generated interactive API documentation (Swagger UI - accessible via /docs endpoint), allowing for easy testing and exploration of the /predict endpoint with example JSON inputs.4. Tools & LibrariesProgramming Language: PythonData Manipulation: pandas, numpyMachine Learning: scikit-learn, xgboostModel Persistence: joblibAPI Development: FastAPI, uvicorn, pydanticDeployment Tunneling: pyngrokVisualization: matplotlib, seabornVersion Control: Git, GitHub5. Future Enhancements & LearningsResidual Analysis: A detailed residual analysis was performed to understand the nature of prediction errors, particularly the large discrepancies observed for high-value cars. This identified a need for more nuanced handling of outliers or rare, expensive vehicle types.Potential Improvements:Further investigation into the features of high-error data points to identify unique characteristics or data quality issues.Exploring advanced feature engineering specific to luxury/exotic car segments.Consider a specialized sub-model or different modeling approach for very high-value vehicles if sufficient data for that segment is available.Implementing model monitoring (data drift, concept drift) post-deployment.Exploring model interpretability techniques (SHAP/LIME) for deeper insights.
